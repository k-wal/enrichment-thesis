{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Enrichment for NER with Partial Annotations\n \n Some changes to this notebook are required before running. These changes are mentioned at appropriate places in the notebook. The following need to be changed\n \n * paths when saving/loading checkpoints\n * paths of datasets\n * name of pre-trained BERT model (if not dealing with Material Science corpora)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport os\nimport re\nimport spacy\nfrom tqdm import tqdm\nimport inflect\nimport numpy as np\nimport seaborn as sns\nnlp = spacy.load(\"en_core_web_sm\")\nstopwords = nltk.corpus.stopwords.words('english') \nengine = inflect.engine()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \n\nfrom sklearn.metrics import accuracy_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertConfig, BertForTokenClassification\nfrom transformers import BertTokenizerFast\nfrom transformers import pipeline\nimport torch.nn.functional as F\nfrom seqeval.metrics import classification_report, precision_score, recall_score, f1_score\nimport conlleval\nfrom seqeval.scheme import IOB2\n\ndevice = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:19:03.506633Z","iopub.execute_input":"2023-06-23T19:19:03.507393Z","iopub.status.idle":"2023-06-23T19:19:30.21138Z","shell.execute_reply.started":"2023-06-23T19:19:03.507351Z","shell.execute_reply":"2023-06-23T19:19:30.210319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## reading data","metadata":{}},{"cell_type":"markdown","source":"change the paths of datasets when using\n\ndata files are in csv format, with the two columns having the headers \"sentence\" and \"word_labels\"\n\nsentence: tokens separated by space (' ')\nword_labels: B/I/O tags for tokens separated by commas (',')","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/elsevier-tagged-data/matscholar_train_bio_partial.csv')\ntest_data = pd.read_csv('/kaggle/input/elsevier-tagged-data/matscholar_test_bio.csv')\ntest_data['sentence'] = test_data['sentence'].astype(str)\ndev_data = pd.read_csv('/kaggle/input/elsevier-tagged-data/matscholar_dev_bio.csv')\ndev_data['sentence'] = dev_data['sentence'].astype(str)\n\nadditional_data = pd.read_csv('/kaggle/input/elsevier-tagged-data/matpro_bio_tags.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:21:47.742966Z","iopub.execute_input":"2023-06-23T19:21:47.743393Z","iopub.status.idle":"2023-06-23T19:21:47.847263Z","shell.execute_reply.started":"2023-06-23T19:21:47.743361Z","shell.execute_reply":"2023-06-23T19:21:47.846297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = test_data.dropna()\ndev_data = dev_data.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:21:51.52078Z","iopub.execute_input":"2023-06-23T19:21:51.521752Z","iopub.status.idle":"2023-06-23T19:21:51.530445Z","shell.execute_reply.started":"2023-06-23T19:21:51.521713Z","shell.execute_reply":"2023-06-23T19:21:51.52932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## preparing data loader","metadata":{}},{"cell_type":"markdown","source":"change pretrained BERT model below, depending on which domain the datasets belong to","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 256\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 1\nEPOCHS = 3\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\ntokenizer = BertTokenizer.from_pretrained('m3rg-iitd/matscibert')\nfast_tokenizer = BertTokenizerFast.from_pretrained('m3rg-iitd/matscibert')","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:21:52.604966Z","iopub.execute_input":"2023-06-23T19:21:52.605824Z","iopub.status.idle":"2023-06-23T19:21:54.168822Z","shell.execute_reply.started":"2023-06-23T19:21:52.605783Z","shell.execute_reply":"2023-06-23T19:21:54.167868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label2id = {'B':0, 'I':1, 'O':2}\nid2label = {0:'B', 1:'I', 2:'O'}","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:21:58.823334Z","iopub.execute_input":"2023-06-23T19:21:58.823722Z","iopub.status.idle":"2023-06-23T19:21:58.829915Z","shell.execute_reply.started":"2023-06-23T19:21:58.823689Z","shell.execute_reply":"2023-06-23T19:21:58.828803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n    \"\"\"\n    Word piece tokenization makes it difficult to match word labels\n    back up with individual word pieces. This function tokenizes each\n    word one at a time so that it is easier to preserve the correct\n    label for each subword. It is, of course, a bit slower in processing\n    time, but it will help our model achieve higher accuracy.\n    \"\"\"\n\n    tokenized_sentence = []\n    labels = []\n\n    sentence = sentence.strip()\n    try:\n        l = text_labels.split(',')\n    except:\n        print(text_labels)\n    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n\n        # Tokenize the word and count # of subwords the word is broken into\n        tokenized_word = tokenizer.tokenize(word)\n        n_subwords = len(tokenized_word)\n\n        # Add the tokenized word to the final tokenized word list\n        tokenized_sentence.extend(tokenized_word)\n\n        # Add the same label to the new list of labels `n_subwords` times\n        labels.extend([label] * n_subwords)\n\n    return tokenized_sentence, labels","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:22:00.738564Z","iopub.execute_input":"2023-06-23T19:22:00.738936Z","iopub.status.idle":"2023-06-23T19:22:00.746994Z","shell.execute_reply.started":"2023-06-23T19:22:00.738905Z","shell.execute_reply":"2023-06-23T19:22:00.745784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n        # step 1: tokenize (and adapt corresponding labels)\n        sentence = self.data.sentence[index]  \n        word_labels = self.data.word_labels[index]  \n        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n        # step 2: add special tokens (and corresponding labels)\n        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n        labels.insert(0, \"O\") # add outside label for [CLS] token\n        labels.insert(-1, \"O\") # add outside label for [SEP] token\n\n        # step 3: truncating/padding\n        maxlen = self.max_len\n\n        if (len(tokenized_sentence) > maxlen):\n          # truncate\n          tokenized_sentence = tokenized_sentence[:maxlen]\n          labels = labels[:maxlen]\n        else:\n          # pad\n          tokenized_sentence = tokenized_sentence + ['[PAD]' for _ in range(maxlen - len(tokenized_sentence))]\n          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n\n        # step 4: obtain the attention mask\n        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n        \n        # step 5: convert tokens to input ids\n        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n\n        label_ids = [label2id[label] for label in labels]\n        # the following line is deprecated\n        #label_ids = [label if label != 0 else -100 for label in label_ids]\n        \n        return {\n              'ids': torch.tensor(ids, dtype=torch.long),\n              'mask': torch.tensor(attn_mask, dtype=torch.long),\n              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n              'targets': torch.tensor(label_ids, dtype=torch.long),\n        } \n    \n    def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:22:03.419637Z","iopub.execute_input":"2023-06-23T19:22:03.419993Z","iopub.status.idle":"2023-06-23T19:22:03.431067Z","shell.execute_reply.started":"2023-06-23T19:22:03.419964Z","shell.execute_reply":"2023-06-23T19:22:03.429786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"training data (partial): {}\".format(train_data.shape))\nprint(\"test data: {}\".format(test_data.shape))\nprint(\"dev data: {}\".format(dev_data.shape))\nprint(\"additional data: {}\".format(additional_data.shape))\n\ntraining_set = dataset(train_data, tokenizer, MAX_LEN)\ntesting_set = dataset(test_data, tokenizer, MAX_LEN)\ndev_set = dataset(dev_data, tokenizer, MAX_LEN)\nadditional_set = dataset(additional_data, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:22:44.754952Z","iopub.execute_input":"2023-06-23T19:22:44.75532Z","iopub.status.idle":"2023-06-23T19:22:44.764465Z","shell.execute_reply.started":"2023-06-23T19:22:44.755291Z","shell.execute_reply":"2023-06-23T19:22:44.763545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)\ndev_loader = DataLoader(dev_set, **test_params)\nadditional_loader = DataLoader(additional_set, **train_params)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:23:04.506505Z","iopub.execute_input":"2023-06-23T19:23:04.506987Z","iopub.status.idle":"2023-06-23T19:23:04.515129Z","shell.execute_reply.started":"2023-06-23T19:23:04.506952Z","shell.execute_reply":"2023-06-23T19:23:04.513894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model definition","metadata":{}},{"cell_type":"markdown","source":"change model name below, depending on domain","metadata":{}},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained('m3rg-iitd/matscibert', \n                                                   num_labels=len(id2label),\n                                                   id2label=id2label,\n                                                   label2id=label2id)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:23:58.5312Z","iopub.execute_input":"2023-06-23T19:23:58.531685Z","iopub.status.idle":"2023-06-23T19:24:08.106186Z","shell.execute_reply.started":"2023-06-23T19:23:58.531638Z","shell.execute_reply":"2023-06-23T19:24:08.105223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## training function","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:24:08.108162Z","iopub.execute_input":"2023-06-23T19:24:08.109211Z","iopub.status.idle":"2023-06-23T19:24:08.117491Z","shell.execute_reply.started":"2023-06-23T19:24:08.109174Z","shell.execute_reply":"2023-06-23T19:24:08.11454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the training function on the 80% of the dataset for tuning the bert model\ndef train(epoch, loader):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    tr_preds, tr_labels = [], []\n    # put model in training mode\n    model.train()\n    \n    for idx, batch in enumerate(loader):\n        \n        ids = batch['ids'].to(device, dtype = torch.long)\n        mask = batch['mask'].to(device, dtype = torch.long)\n        targets = batch['targets'].to(device, dtype = torch.long)\n\n        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n        loss, tr_logits = outputs.loss, outputs.logits\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += targets.size(0)\n        \n        if idx % 500==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss per 500 training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n        targets = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tr_preds.extend(predictions)\n        tr_labels.extend(targets)\n        \n        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:24:08.119197Z","iopub.execute_input":"2023-06-23T19:24:08.119529Z","iopub.status.idle":"2023-06-23T19:24:08.142485Z","shell.execute_reply.started":"2023-06-23T19:24:08.119499Z","shell.execute_reply":"2023-06-23T19:24:08.141529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## testing function","metadata":{}},{"cell_type":"code","source":"# returns labels, predictions, tokens and probabilities\ndef test(model, testing_loader):\n    # put model in evaluation mode\n    model.eval()\n    \n    nb_eval_examples, nb_eval_steps = 0, 0\n    eval_preds, eval_labels = [], []\n    eval_tokens = []\n    eval_probs = []\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(tqdm(testing_loader)):\n            \n            ids = batch['ids'].to(device, dtype = torch.long)\n            mask = batch['mask'].to(device, dtype = torch.long)\n            targets = batch['targets'].to(device, dtype = torch.long)\n            \n            outputs = model(input_ids = ids, attention_mask=mask)\n            eval_logits = outputs.logits\n            \n            nb_eval_steps += 1\n            nb_eval_examples += targets.size(0)\n              \n            # compute evaluation accuracy\n            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n            # applying softmax to get probabilities from logits\n            probabilities = F.softmax(active_logits, dim=-1)\n            flattened_probs = torch.max(probabilities, axis=1)[0]\n            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n            targets = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n            probs = [p.item() for p in torch.masked_select(flattened_probs, active_accuracy)]\n            \n            # convert tuples of tokens into a flattened list\n            flattened_ids = ids.view(-1)\n            ids = torch.masked_select(flattened_ids, active_accuracy)\n            tokens = tokenizer.convert_ids_to_tokens(ids)\n            \n            \n            eval_labels.append(targets)\n            eval_preds.append(predictions)\n            eval_tokens.append(tokens)\n            eval_probs.append(probs)\n            \n    labels, predictions = [], []\n    for l in eval_labels:\n        labels.append([id2label[id.item()] for id in l])\n\n    for p in eval_preds:\n        predictions.append([id2label[id.item()] for id in p])\n\n    return labels, predictions, eval_tokens, eval_probs","metadata":{"execution":{"iopub.status.busy":"2023-06-23T19:24:08.510609Z","iopub.execute_input":"2023-06-23T19:24:08.511712Z","iopub.status.idle":"2023-06-23T19:24:08.525463Z","shell.execute_reply.started":"2023-06-23T19:24:08.511667Z","shell.execute_reply":"2023-06-23T19:24:08.524539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## step 1: training on additional dataset","metadata":{}},{"cell_type":"markdown","source":"change path to which model checkpoints are saved","metadata":{}},{"cell_type":"code","source":"#training the model on matpro data\n# for epoch in range(10):\n#    print(f\"Training epoch: {epoch + 1}\")\n#    train(epoch, additional_set)\n   \n#    # test as it goes on\n#    print(\"DEV TESTING\")\n#    labels, predictions, tokens, probs = test(model, dev_loader)\n#    print(classification_report(labels, predictions, mode=\"strict\", scheme=IOB2))\n    \n   # #save the model    \n#    state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n#                  'optimizer': optimizer.state_dict()}\n\n#    try:\n#        name = '/kaggle/working/conll-matsci-' + str(epoch) + '-checkpoint.pt'\n#        torch.save(state, name)\n#    except:\n#        pass","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.81979Z","iopub.status.idle":"2023-06-20T10:49:26.820532Z","shell.execute_reply.started":"2023-06-20T10:49:26.820243Z","shell.execute_reply":"2023-06-20T10:49:26.820268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## load checkpoint","metadata":{}},{"cell_type":"markdown","source":"change checkpoint path","metadata":{}},{"cell_type":"code","source":"def load_checkpoint(model, optimizer, filepath):\n    start_epoch = 0\n    if os.path.isfile(filepath):\n        print(\"=> loading checkpoint '{}'\".format(filepath))\n        checkpoint = torch.load(filepath)\n        start_epoch = checkpoint['epoch']\n        model.load_state_dict(checkpoint['state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        print(\"=> loaded checkpoint '{}' (epoch {})\"\n                  .format(filepath, checkpoint['epoch']))\n    else:\n        print(\"=> no checkpoint found at '{}'\".format(filepath))\n\n    return model, optimizer, start_epoch","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.824896Z","iopub.status.idle":"2023-06-20T10:49:26.825409Z","shell.execute_reply.started":"2023-06-20T10:49:26.82514Z","shell.execute_reply":"2023-06-20T10:49:26.825162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath = '/kaggle/input/checkpoints/matpro-2-checkpoint.pt'\nmodel, optimizer, start_epoch = load_checkpoint(model, optimizer, filepath)\n\n# now individually transfer the optimizer parts...\nfor state in optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.826941Z","iopub.status.idle":"2023-06-20T10:49:26.827811Z","shell.execute_reply.started":"2023-06-20T10:49:26.827511Z","shell.execute_reply":"2023-06-20T10:49:26.827536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## testing over dev set","metadata":{}},{"cell_type":"code","source":"labels, predictions, tokens, probs = test(model, dev_loader)","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.833406Z","iopub.status.idle":"2023-06-20T10:49:26.834286Z","shell.execute_reply.started":"2023-06-20T10:49:26.833995Z","shell.execute_reply":"2023-06-20T10:49:26.834022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.scheme import IOB2\n# using seqeval library to get metrics for entities instead of tokens\nprint(classification_report(labels, predictions, mode=\"strict\", scheme=IOB2))","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.835846Z","iopub.status.idle":"2023-06-20T10:49:26.836755Z","shell.execute_reply.started":"2023-06-20T10:49:26.836479Z","shell.execute_reply":"2023-06-20T10:49:26.836505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lines = []\nfor l, p in zip(labels, predictions):\n    for label, pred in zip(l, p):\n        lines.append('word ' + label + ' ' + pred)\nlines = '\\n'.join(lines).splitlines()\n\nres = conlleval.evaluate(lines)\nprint(conlleval.report(res))","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.83833Z","iopub.status.idle":"2023-06-20T10:49:26.83923Z","shell.execute_reply.started":"2023-06-20T10:49:26.838946Z","shell.execute_reply":"2023-06-20T10:49:26.838973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## combination function (for labels and predictions) ","metadata":{}},{"cell_type":"code","source":"# input: tokens, labels, predictions, probs for a training instance\n# tokens = list of tokens in the instance\n# labels = list of labels for the instance, from the partially annotated set\n# predictions = list of predictions for the instance\n# probs = list of likelihoods for the predictions\n\n#output: added, sentence, new_labels\n# added: number of entities added to labels \n# sentence: list of tokens\n# new_labels: enriched labels\ndef combine_labels_predictions_sentence(tokens, labels, predictions, probs, threshold=0.50):\n    sentence = []\n    new_labels = []\n    last_label = 'O'\n    added = 0\n    \n    for token,label,pred,prob in zip(tokens, labels, predictions, probs):\n        if len(new_labels) == 0:\n            last_label = 'O'\n        else:\n            last_label = new_labels[-1]\n        \n        # if token is one of bert added tokens, continue\n        if token=='[CLS]' or token=='[SEP]':\n            continue\n            \n        # if label and prediction are same, simply add token and label\n        if label == pred:\n            sentence.append(token)\n            new_labels.append(label)\n            continue\n            \n        # if label is entity, ignore prediction\n        if label == 'B' or label == 'I':\n            sentence.append(token)\n            new_labels.append(label)\n            continue\n\n        # if the label is not entity but prediction is entity\n        if label != pred and label == 'O':\n            # if probability of prediction is less than threshold, ignore\n            if prob < threshold:\n                sentence.append(token)\n                new_labels.append(label)\n                continue\n                \n            # if prediction is I and last token was marked B, assume part of the same entity\n            if pred == 'I' and last_label == 'B':\n                sentence.append(token)\n                new_labels.append(pred)\n                continue\n            \n            # if prediction is I and last token was O, assume new entity started\n            if pred == 'I' and last_label == 'O':\n                added += 1\n                sentence.append(token)\n                new_labels.append('B')\n                continue\n            \n            # if prediction is B then assume its a start of new entity\n            if pred == 'B':\n                added += 1\n                sentence.append(token)\n                new_labels.append(pred)\n                continue\n\n    return added, sentence, new_labels","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.8435Z","iopub.status.idle":"2023-06-20T10:49:26.844415Z","shell.execute_reply.started":"2023-06-20T10:49:26.844123Z","shell.execute_reply":"2023-06-20T10:49:26.844149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predicted labels and call combination functions, return dataframe with new data\ndef get_combined_dataset(model, loader):\n    labels, predictions, tokens, probs = test(model, loader)\n    df = pd.DataFrame(columns=['sentence', 'word_labels'])\n    added = 0\n    for l, pred, t, prob in tqdm(zip(labels, predictions, tokens, probs)):\n        cur_added, sentence, new_labels = combine_labels_predictions_sentence(t, l, pred, prob)\n        added += cur_added\n        sentence = ' '.join(sentence)\n        word_labels = ','.join(new_labels)\n        df = df.append({'sentence':sentence, 'word_labels':word_labels}, ignore_index=True)\n    print(\"New Entities Added: \", added)\n    return df\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.845988Z","iopub.status.idle":"2023-06-20T10:49:26.8469Z","shell.execute_reply.started":"2023-06-20T10:49:26.846612Z","shell.execute_reply":"2023-06-20T10:49:26.846638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_train_data = train_data\nupdated_training_loader = training_loader\nupdated_training_set = training_set","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.848492Z","iopub.status.idle":"2023-06-20T10:49:26.849389Z","shell.execute_reply.started":"2023-06-20T10:49:26.849101Z","shell.execute_reply":"2023-06-20T10:49:26.849127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## enrich and fine-tune","metadata":{}},{"cell_type":"markdown","source":"change checkpoint path","metadata":{}},{"cell_type":"code","source":"# for each epoch, train on \ndev_results = []\nfor epoch in range(5):\n   # updating training data to predict+add new stuff\n   partial_loader_testing = DataLoader(updated_training_set, **test_params)\n   updated_train_data = get_combined_dataset(model, partial_loader_testing)\n   updated_training_set = dataset(updated_train_data, tokenizer, MAX_LEN)\n   updated_training_loader = DataLoader(updated_training_set, **train_params)\n   print(\"New training data (enriched): {}\".format(updated_train_data.shape))\n    \n   # now train\n   print(f\"Training epoch: {epoch + 1}\")\n   train(epoch, updated_training_loader)\n   \n   \n   # test as it goes on, and save dev results\n   print(\"DEV TESTING\")\n   labels, predictions, tokens, probs = test(model, dev_loader)\n   precision = precision_score(labels, predictions, mode=\"strict\", scheme=IOB2)\n   recall = recall_score(labels, predictions, mode=\"strict\", scheme=IOB2)\n   f1 = f1_score(labels, predictions, mode=\"strict\", scheme=IOB2)\n   dev_results.append({'precision':precision, 'recall':recall, 'f1':f1, 'epoch':epoch+1})\n   print(classification_report(labels, predictions, mode=\"strict\", scheme=IOB2))\n\n   #save the model    \n   torch.save(model, '/kaggle/working/matscholar-matpro-enriched-model-'+str(epoch+1)+'.pt')","metadata":{"execution":{"iopub.status.busy":"2023-06-20T10:49:26.850968Z","iopub.status.idle":"2023-06-20T10:49:26.851911Z","shell.execute_reply.started":"2023-06-20T10:49:26.851631Z","shell.execute_reply":"2023-06-20T10:49:26.851658Z"},"trusted":true},"execution_count":null,"outputs":[]}]}